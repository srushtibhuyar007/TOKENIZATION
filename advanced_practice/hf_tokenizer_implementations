
### Pipeline Components

- **Normalizer**  
  Handles Unicode normalization, casing, accent removal, and script cleanup.

- **PreTokenizer**  
  Performs initial splitting (whitespace, regex-based, or byte-level).

- **Tokenizer Model**  
  Applies BPE, WordPiece, or Unigram logic.

- **PostProcessor**  
  Injects special tokens such as `[CLS]`, `[SEP]`, or sequence separators.

---

## 1.4 Custom Tokenizer Training (Hugging Face)

Hugging Face allows training **domain-specific tokenizers** for specialized datasets such as medical text, legal documents, or source code.

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()

trainer = trainers.BpeTrainer(
    vocab_size=32000,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]"]
)

tokenizer.train(files=["data.txt"], trainer=trainer)
